'''Jianyu ChenThis is the code for the optimal control Especially for the Maier-Stein System'''#%%import torchfrom torch import nnfrom torch.nn import functional as Ffrom operator import add import mathimport numpy as npimport matplotlib.pyplot as plt#%%t = np.arange(100) * 0.01print(t)ux = np.random.normal(8, 3, 100)#mean and 标准差#uy = np.linspace(0, 0.5, 100)uy = np.random.normal(0, 0.08, 100)a = zip(ux, uy)print(list(a))u = [list(aa) for aa in a]print(u)#%%eps = 0.4gamma = 5 def runge_kutta_x(x, t, dt, f, u, i):    """   x is the initial value for x             t is the initial value for t          dt is the time step in t          f is derivative of function x(t)    """    sigma = 1+eps*(x[0]**2)    u1_i = sigma*u[i][0]     u2_i = u[i][1]    u_i = [u1_i, u2_i]    k1 = dt * np.array(list(map(add, f(x, t), u_i)))        u1_i1 = sigma*u[i+1][0]     u2_i2 = u[i+1][1]    u_i = [u1_i1, u2_i2]    k2 = dt * np.array(list(map(add, f(x + k1, t+dt), u_i)))    return x + (k1 + k2) / 2.def get_x(u):    t = 0.      x = (-1, 0)      T = 1.0     dt = 0.01    xs, ts = [x], [0.0]    def func(x, t):        x1 = x[0]        x2 = x[1]        return [x1-x1**3-gamma*x1*x2**2, -(1+x1**2)*x2]    i = 0    while i < 99:        x = runge_kutta_x(x,t, dt, func, u, i)        i += 1        t += dt        xs.append(x)        ts.append(t)    return np.array([np.float32(xx) for xx in xs])#%%x = get_x(u)x = torch.tensor(x).to(torch.float32)print(x)#%%eps = 0.4gamma = 5 import mathimport numpy as npimport matplotlib.pyplot as pltdef runge_kutta(p1, p2, x, gamma, eps, u, dt, f, i):    """   x is the initial value for x             t is the initial value for t          dt is the time step in t          f is derivative of function x(t)    """        x_now = x[i][0]    y_now = x[i][1]    u_now = u[i]    x_next = x[i-1][0]    y_next = x[i-1][1]    u_next = u[i-1]    k1 = (-1.0) * dt * np.array(f(p1, p2, x_now, y_now, gamma, eps, u_now))    #print('k1', k1)    k2 = (-1.0) * dt * np.array(f(p1 + k1[0], p2 + k1[1], x_next, y_next, gamma, eps, u_next))    return np.array([p1, p2]) + (k1 + k2) / 2.def get_p(x, u, gamma, eps):    # x whole sample trajectory //         t = 0.      p1 = -2*(x[99][0]-1)/((x[99][0]-1)**2+1)**2    p2 = -2*(x[99][1]-0)/((x[99][1]-0)**2+1)**2    p = (p1, p2)    T = 1.0     dt = 0.01    ps, ts = [p], [0.0]            def func(p1, p2, x, y, gamma, eps, u):        #print('-------', p1*(3*x**2+gamma*y**2-2*ep*x*u[0]-1)+2*p2*x*y-6*x+2*ep**2*x)        p1 = p[0]        p2 = p[1]        G1 = -2*eps*(1+eps*x**2)*(2*x-4*x**3-2*gamma*y**2*x+2*eps*x+4*eps**2*x**3)        G2 = 4*eps**2*(x**3-x**5-gamma*x**3*y**2+eps*x**3+eps**2*x**5)        G = (G1+G2)/(1+eps*x**2)**2        result = (p1*(3*x**2+gamma*y**2-2*eps*x*u[0]-1)+2*p2*x*y+2*eps**2*x-2*eps*u[1]-8*x+6*eps**2*x+G,                 2*p1*gamma*x*y+p2*(1+x**2)-2*gamma*y)        return result            i = 99    while i > 0 :        #print ("i, t", i, t)        p = runge_kutta(p1, p2, x, gamma, eps, u, dt, func, i)        i -= 1        t -= dt        ps.append(p)        ts.append(t)    return np.array([np.float32(pp) for pp in ps])#%%p = get_p(x, u, gamma, eps)reversed_p = []for i in reversed(p):    reversed_p.append(i)    p = reversed_pp = torch.tensor(p).to(torch.float32)print(p)#%%class MLP(nn.Module):     def __init__(self):#call the constractor class"Module" to perform necessary initialization.        super().__init__()        self.hidden1 = nn.Linear(1, 8)#hidden        self.hidden2 = nn.Linear(8, 16)#hidden        self.out = nn.Linear(16,2)#out put layer #define the forward propagation of the model,how to return the required model output  #based on the input "t"（input t is "u"）      def forward(self, u):        u = self.hidden1(u)        u = F.relu(u)        u = self.hidden2(u)        u = F.relu(u)        out = self.out(u)        return outnet = MLP()#%%t = torch.tensor(t)t = t.to(torch.float32)p = get_p(x, u, gamma, eps)reversed_p = []for i in reversed(p):    reversed_p.append(i)    p = reversed_px = torch.tensor(x).to(torch.float32)p = torch.tensor(p).to(torch.float32)#%%print(x)print(p)#%%import numpy as npimport matplotlib.pylab as pltLoss = []#lr =0.01epoch = 1000N = 45#iteration timesar=np.array(x)#print(ar)x_1 = ar[:,0]#print(ar[:,0])x_2 = ar[:,1]#print('---------------x_1', x_1)#print('---------------x_2', x_2)ar2=np.array(p)p_1 = ar2[:,0]#print(ar[:,0])p_2 = ar2[:,1]Q = 0.05*((x_1[99]-1)**2+(x_2[99]-0)**2)print('------Q', Q)dt = 0.01dx= (x[1:]-x[0:-1])/dtdp = (p[1:]-p[0:-1])/dtrho = 0.001 #修正系数updater = torch.optim.SGD(net.parameters(), 0.001)#%%def train():        for ii in range(epoch):        H = 0           for j in range(99):             t_j = t[j].reshape(1)             u_hat_j = net(torch.tensor(t_j))            u1 = u_hat_j[0]            u2 = u_hat_j[1]            x_j = torch.tensor(x_1[j].reshape(1))        #print('----------x_j', x_j)            y_j = torch.tensor(x_2[j].reshape(1))        #print('----------y_j', y_j)            p1_j = torch.tensor(p_1[j].reshape(1))        #print('----------p1_j', p1_j)            p2_j = torch.tensor(p_2[j].reshape(1))        #print('----------p2_j', p2_j)            G1 = -2*eps*(1+eps*x_j**2)*(2*x_j-4*x_j**3-2*gamma*y_j**2*x_j+2*eps*x_j+4*eps**2*x_j**3)            G2 = 4*eps**2*(x_j**3-x_j**5-gamma*x_j**3*y_j**2+eps*x_j**3+eps**2*x_j**5)            G = 2*eps**2*x_j-2*eps*u1-8*x_j+6*eps**2*x_j+(G1+G2)/(1+eps*x_j**2)**2            A = p1_j * (x_j-x_j**3-gamma*x_j*y_j**2+(1+eps*x_j**2)*u1)         #print('-------A', A)            B = p2_j * (-(1+x_j**2)*y_j+u2)            divb = -4*x_j**2-gamma*y_j**2+eps+3*eps**2*x_j**2+((-2*eps)*(x_j**2-x_j**4-gamma*x_j**2*y_j**2+eps*x_j**2+eps**2*x_j**4))/(1+eps*x_j**2)        #print('--------B', B)            C = (u1**2+u2**2)+eps**2*x_j**2-2*eps*x_j*u1+divb        #print('--------C', C)            #argumented Hamiltonian            R1 = (dx[j][0]-(x_j-x_j**3-gamma*x_j*y_j**2+(1+eps*x_j**2)*u1))**2+(dx[j][1]+(1+x_j**2)*y_j-u2)**2            R2 = (dp[j][0]- (p1_j*(3*x_j**2+gamma*y_j**2-2*eps*x_j*u1-1)+2*p2_j*x_j*y_j+G))**2+(dp[j][1]-(2*p1_j*gamma*x_j*y_j+p2_j*(1+x_j**2)-2*gamma*y_j)**2)                       R = 0.5*rho*(R1+R2)        #print('--------R', R)            H = A+B-C-R        #print('---------------H', H)        #print('%%%%%%%%', H-Q)                l = -(H-Q)/100 # Q的值不能太大，不然影响loss的权重占比过多，系数取0.5        #print('---------------l', l)           updater.zero_grad()        l.backward()        updater.step()        Loss.append(l.detach().item())            return#%%for k in range(N):    train()    print(k)    #UU = [net(torch.tensor(t_j.reshape(1))) for t_j in t]    #u_op = [u.detach().numpy() for u in UU]    U = [net(torch.tensor(t_j.reshape(1))) for t_j in t]    UU = []    for ii in range(len(U)):        #u = [U1[ii].detach().numpy(), U2[ii].detach().numpy()]        u = [U[ii].detach().numpy()]    #print("------u", u[0][0], u[1][0])        #UU.append([u[0][0], u[1][0]])        UU.append(u[0])    #print(UU)        u_op = UU    #print("^^^^^^^^u_op", u_op)    x = get_x(u_op)    p = get_p(x, u_op, gamma, eps)    reversed_p = []    for i in reversed(p):        reversed_p.append(i)    p = reversed_p#%%x = range(epoch*N)l = Loss#print(x)plt.title("loss function")plt.xlabel("epoch times")plt.ylabel("loss value")plt.plot(x, l)plt.show()#%%import scipy.io as scioscio.savemat('x.mat',{'x':x})scio.savemat('l.mat',{'l':l})#%%x = ty = u_op#y = list(map(lambda x:x.detach().numpy(), y))plt.title("optimal control")plt.xlabel("Time")plt.ylabel("u")plt.plot(x, y)plt.show()#%%import scipy.io as scioscio.savemat('u_op.mat',{'y':y})